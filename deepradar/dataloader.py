"""Lightning-based dataloader for a collection of Rover traces.

The primary API used should be the `RoverDataModule`, which can be fully
deserialized from a `**config` json-compatible spec. For example::

    dataset = RoverDataModule(**cfg, path="path/to/traces")

Key configuration options for `cfg`:

- **batch_size**: data batch size.
- **path**: all traces should be put in a directory `path`.
- **traces**: traces to use for training/validation.
- **pval**: the last `pval` (`0 < pval < 1`) of each trace is reserved for
  validation, with the first `1 - pval` being used for training.
- **val_samples**: fixed indices from the validation set to render
  visualizations for on each validation round.
- **channels**: channel specifications; see :py:mod:`deepradar.channels`.
- **augmentations**: data augmentation parameter generators. Key correspond to
  possible augmentations; see :py:mod:`deepradar.transforms`.

Example configuration::

    {
        "batch_size": 64,
        "train": ["cic1", "cic2"],
        "pval": 0.2,
        "val_samples": [100, 200, 300, 400],
        "channels": {
            "lidar": {
                "name": "RawChannel", "indices": "lidar",
                "args": {
                    "sensor": "_lidar", "channel": "rng",
                    "transform": [
                        {"name": "Destagger", "args": {}},
                        {"name": "Map2D", "args": {}}]
                }
            },
            "radar": {
                "name": "RawChannel", "indices": "radar",
                "args": {
                    "sensor": "radar", "channel": "iq",
                    "transform": [
                        {"name": "AssertTx2", "args": {}},
                        {"name": "IIQQtoIQ", "args": {}},
                        {"name": "FFTArray",
                         "args": {"pad": 0, "axes": [0, 1, 2, 3]}},
                        {"name": "ComplexPhase", "args": {}}]
                }
            }
        },
        "augmentations": {
            "azimuth_flip": {"name": "Bernoulli", "args": {"p": 0.5}},
            "range_scale": {"name": "TruncatedLogNormal", "args": {
                "p": 0.5, "std": 0.2, "clip": 2.0}}
        }
    }
"""

import multiprocessing
import os
from functools import cached_property

import lightning as L
import numpy as np
from beartype.typing import Any, Callable, Iterable, Optional
from jaxtyping import Shaped
from torch.utils.data import DataLoader, Dataset

from . import augmentations as mod_augmentations
from . import channels as mod_channels
from .channels import Index

#: Augmentation spec generator type
Augmentation = Callable[[], Any]


class RoverTrace:
    """Single rover trace.

    Args:
        path: path to trace.
        indices: indices file relative to `path`; generated by `roverp align`.
        channels: channel specifications; see :py:mod:`deepradar.channels`.
        augment: data augmentation parameter generators; see
            :py:mod:`deepradar.augmentations`.
        bounds: portion of the trace to use; if not specified, the entire trace
            is used. The bounds are specified as fractions of the dataset, e.g.
            `(0.0, 0.8)` for the first 80% of the trace and `(0.8, 1.0)` for
            the last 20%.
    """

    def __init__(
        self, path: str, indices: str = "_fusion/indices.npz",
        channels: dict[str, dict] = {},
        augmentations: dict[str, Callable] = {},
        bounds: Optional[tuple[float, float]] = None
    ) -> None:
        npz = np.load(os.path.join(path, indices))
        self.indices = npz["indices"]
        self.augmentations = augmentations

        if bounds is not None:
            left = int(bounds[0] * self.indices.shape[0])
            right = int(bounds[1] * self.indices.shape[0])
            self.indices = self.indices[left:right]

        channel_indices = {n: i for i, n in enumerate(npz["sensors"])}
        self.channels = {
            k: getattr(mod_channels, spec["name"]).from_config(
                dataset=path,
                indices=self.indices[:, channel_indices[spec["indices"]]],
                **spec["args"])
            for k, spec in channels.items()
        }

    def __len__(self) -> int:
        return self.indices.shape[0]

    def __getitem__(self, idx: Index) -> dict[str, Shaped[np.ndarray, "..."]]:
        aug = {k: v() for k, v in self.augmentations.items()}
        return {k: v.index(idx, aug=aug) for k, v in self.channels.items()}


class RoverData(Dataset):
    """Collection of rover traces.

    Args:
        paths: list of dataset paths to include.
        kwargs: forwarded to each :class:`.RoverTrace`.
    """

    def __init__(self, paths: list[str], **kwargs) -> None:
        self.traces = [RoverTrace(p, **kwargs) for p in paths]
        self.indices = np.concatenate([
            [0], np.cumsum([len(t) for t in self.traces])]) - 1

    def __len__(self) -> int:
        return sum(len(t) for t in self.traces)

    def __getitem__(self, idx: Index) -> dict[str, Shaped[np.ndarray, "..."]]:
        if idx < 0 or idx > self.indices[-1]:
            raise ValueError("Index out of bounds.")
        else:
            ii = np.searchsorted(self.indices, idx) - 1
            return self.traces[ii][idx - self.indices[ii] - 1]


class RoverDataModule(L.LightningDataModule):
    """Rover dataloaders.

    Args:
        path: base path (directory containing datasets).
        traces: list of traces to be used in training.
        pval: proportion of data to use for validation. Each `train` trace is
            split into two parts, with the first `1 - pval` being used for
            training, and the last `pval` being used for validation.
        ptrain: If specified, overrides `1 - pval` as the proportion used for
            training (e.g. to implement dataset size ablations).
        batch_size: train batch size.
        val_samples: indices of (val) data to use as visualization samples. If
            `val_samples: int`, uses evenly spaced samples from the validation
            set (i.e. `linspace(0, len(val) - 1, val_samples)`).
        channels: channel specifications; see :py:mod:`deepradar.channels`.
        augmentations: data augmentation spec generators; see
            :py:mod:`deepradar.augmentations`.
        debug: whether to run in debug mode. When `debug=True`, uses
            `num_workers=0` (run dataloaders in main thread) to allow debuggers
            to work properly; otherwise, uses `num_workers=nproc`.
    """

    def __init__(self,
        path: str, traces: list[str] = [], pval: float = 0.2,
        ptrain: Optional[float] = None,
        batch_size: int = 64, val_samples: int | Iterable[int] = 16,
        channels: dict[str, dict] = {},
        augmentations: dict[str, dict] = {}, debug: bool = False
    ) -> None:
        super().__init__()
        self.base = path
        self.traces = traces
        self.pval = pval
        self.ptrain = (1 - pval if ptrain is None else ptrain)

        self._augmentations = {
            k: getattr(mod_augmentations, v["name"])(**v["args"])
            for k, v in augmentations.items()}
        self._paths = [os.path.join(self.base, t) for t in self.traces]
        self._channels = channels

        self.batch_size = batch_size
        self.nproc = 0 if debug else multiprocessing.cpu_count()
        self._val_samples = val_samples

    def train_dataloader(self) -> DataLoader:
        """Get train dataloader (lightning API).

        Returns:
            Single training dataloader, with all traces batched and shuffled.
        """
        ds = RoverData(
            self._paths, channels=self._channels,
            augmentations=self._augmentations, bounds=(0.0, self.ptrain))
        return DataLoader(
            ds, batch_size=self.batch_size, shuffle=True, drop_last=True,
            num_workers=self.nproc, pin_memory=True)

    def val_dataloader(self) -> DataLoader:
        """Get val dataloader (lightning API).

        Returns:
            Single validation dataloader, with all traces concatenated.
        """
        ds = RoverData(
            self._paths, channels=self._channels, augmentations={},
            bounds=(1.0 - self.pval, 1.0))
        return DataLoader(
            ds, batch_size=self.batch_size, shuffle=False, drop_last=True,
            num_workers=self.nproc, pin_memory=True)

    def eval_dataloader(self, path: str, batch_size: int = 16) -> DataLoader:
        """Create evaluation dataloader.

        Args:
            path: path to evaluation trace, relative to the base path.
            batch_size: batch size to use.

        Returns:
            Dataloader for this evaluation trace.
        """
        ds = RoverData(
            [os.path.join(self.base, path)],
            channels=self._channels, augmentations={})
        return DataLoader(
            ds, batch_size=batch_size, shuffle=False, drop_last=False,
            num_workers=self.nproc, pin_memory=True)

    @cached_property
    def val_samples(self) -> dict[str, Shaped[np.ndarray, "..."]]:
        """Get specific validation samples for validation visualizations.

        Returns:
            Data samples in the same batch format as normal dataloading, with
            the specified indices.
        """
        ds = RoverData(
            self._paths, channels=self._channels, augmentations={},
            bounds=(1.0 - self.pval, 1.0))
        if isinstance(self._val_samples, int):
            self._val_samples = np.linspace(
                0, len(ds) - 1, self._val_samples, dtype=np.uint32)

        samples = [ds[i] for i in self._val_samples]
        return {k: np.stack([s[k] for s in samples]) for k in samples[0]}
