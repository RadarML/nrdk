"""Lightning-based dataloader for a collection of Rover traces.

The primary API used should be the `RoverDataModule`, which can be fully
deserialized from a `**config` json-compatible spec. For example::

    dataset = RoverDataModule(**cfg, path="path/to/traces")

Key configuration options for `cfg`:

- **batch_size**: data batch size.
- **path**: all traces should be put in a directory `path`.
- **traces**: traces to use for training/validation.
- **pval**: the last `pval` (`0 < pval < 1`) of each trace is reserved for
  validation, with the first `1 - pval` being used for training.
- **val_samples**: fixed indices from the validation set to render
  visualizations for on each validation round.
- **channels**: channel specifications; see :py:mod:`deepradar.channels`.
- **augmentations**: data augmentation parameter generators. Key correspond to
  possible augmentations; see :py:mod:`deepradar.transforms`.

Example configuration::

    {
        "batch_size": 64,
        "train": ["cic1", "cic2"],
        "pval": 0.2,
        "val_samples": [100, 200, 300, 400],
        "channels": {
            "lidar": {
                "name": "RawChannel", "indices": "lidar",
                "args": {
                    "sensor": "_lidar", "channel": "rng",
                    "transform": [
                        {"name": "Destagger", "args": {}},
                        {"name": "Map2D", "args": {}}]
                }
            },
            "radar": {
                "name": "RawChannel", "indices": "radar",
                "args": {
                    "sensor": "radar", "channel": "iq",
                    "transform": [
                        {"name": "AssertTx2", "args": {}},
                        {"name": "IIQQtoIQ", "args": {}},
                        {"name": "FFTArray",
                         "args": {"pad": 0, "axes": [0, 1, 2, 3]}},
                        {"name": "ComplexPhase", "args": {}}]
                }
            }
        },
        "augmentations": {
            "azimuth_flip": {"name": "Bernoulli", "args": {"p": 0.5}},
            "range_scale": {"name": "TruncatedLogNormal", "args": {
                "p": 0.5, "std": 0.2, "clip": 2.0}}
        }
    }
"""

import multiprocessing
import os
from functools import cached_property

import lightning as L
import numpy as np
import torch
from beartype.typing import Any, Callable, Iterable, Optional
from jaxtyping import Shaped
from torch.utils.data import DataLoader, Dataset

from . import augmentations as mod_augmentations
from . import channels as mod_channels
from .channels import Index

#: Augmentation spec generator type
Augmentation = Callable[[], Any]


class RoverTrace:
    """Single rover trace.

    An `index_file` maps data tuple indices to sensor absolute indices; a
    `mask`, `bounds`, and implicit history requirements associated with a
    history window specify which entries of the index are included in this
    trace when used in the dataloader.

    These criteria are handled in the following order:

        1. If a `mask` file path (relative to the dataset root) is specified,
           load the `mask` entry from this `.npz` file, and apply it to the
           indices. The mask can be a boolean mask or a list of indices into
           the root index file.
        2. If `bounds` are specified, the remaining indices are filtered
           based on the specified fraction.
        3. If any of the channels specify a `window` with nonzero `past` (see
           :py:class:`.channels.Channel`), any entries at the start without at
           least `past` historical samples (i.e. the sensor data index is less
           than `past`) are excluded.

    Args:
        path: path to trace.
        index_file: index file relative to `path`; generated by `roverp align`.
        mask: optional validity mask file to apply to the indices.
        channels: channel specifications; see :py:mod:`deepradar.channels`.
            Note that if the `past` window size is greater than zero, indices
            at the start where any of the sensors have insufficient past data
            are excluded.
        augment: data augmentation parameter generators; see
            :py:mod:`deepradar.augmentations`.
        bounds: portion of the trace to use; if not specified, the entire trace
            is used. The bounds are specified as fractions of the dataset, e.g.
            `(0.0, 0.8)` for the first 80% of the trace and `(0.8, 1.0)` for
            the last 20%.
    """

    def __init__(
        self, path: str, index_file: str = "_fusion/indices.npz",
        mask: Optional[str] = None,
        channels: dict[str, dict] = {},
        augmentations: dict[str, Callable] = {},
        bounds: Optional[tuple[float, float]] = None
    ) -> None:
        npz = np.load(os.path.join(path, index_file))
        indices = npz["indices"]
        channel_indices = {n: i for i, n in enumerate(npz["sensors"])}

        # 1. Apply "valid mask" if specified.
        if mask is not None:
            valid_mask = np.load(os.path.join(path, mask))["mask"]
            indices = indices[valid_mask]

        # 2. Apply percentile bounds, e.g. for data splits.
        if bounds is not None:
            left = int(bounds[0] * indices.shape[0])
            right = int(bounds[1] * indices.shape[0])
            indices = indices[left:right]

        # 3. Handle edge case where there isn't sufficient data for the
        #    specified history length.
        for spec in channels.values():
            if "window" in spec["args"] and spec["args"]["window"] is not None:
                past, _ = spec["args"]["window"]
                ii = channel_indices[spec["indices"]]
                # e.g. if past=1, need indices[0, ii] >= 1.
                if indices[0, ii] < past:
                    skip = np.argmax(indices[:, ii] >= past)
                    indices = indices[skip:, :]

        def _get_indices(spec):
            if spec["indices"] is None:
                return None
            else:
                return indices[:, channel_indices[spec["indices"]]]

        self.indices = indices
        self.augmentations = augmentations
        self.channels = {
            k: getattr(mod_channels, spec["name"]).from_config(
                dataset=path, indices=_get_indices(spec), **spec["args"])
            for k, spec in channels.items()
        }

    def __len__(self) -> int:
        return self.indices.shape[0]

    def __getitem__(self, idx: Index) -> dict[str, Shaped[np.ndarray, "..."]]:
        aug = {k: v() for k, v in self.augmentations.items()}
        return {k: v.index(idx, aug=aug) for k, v in self.channels.items()}


class RoverData(Dataset):
    """Collection of rover traces.

    Args:
        paths: list of dataset paths to include.
        kwargs: forwarded to each :class:`.RoverTrace`.
    """

    def __init__(self, paths: list[str], **kwargs) -> None:
        self.traces = [RoverTrace(p, **kwargs) for p in paths]
        self.indices = np.concatenate([
            [0], np.cumsum([len(t) for t in self.traces])]) - 1

    def __len__(self) -> int:
        return sum(len(t) for t in self.traces)

    def __getitem__(self, idx: Index) -> dict[str, Shaped[np.ndarray, "..."]]:
        if idx < 0 or idx > self.indices[-1]:
            raise ValueError("Index out of bounds.")
        else:
            ii = np.searchsorted(self.indices, idx) - 1
            return self.traces[ii][idx - self.indices[ii] - 1]


class RoverDataModule(L.LightningDataModule):
    """Dataloaders for Rover multimodal data.

    Implementation notes:

    - The input `traces` are sorted (in alphabetical order) prior to loading;
      this order then determines the actual loading order. This ensures that
      the dataset order (which relates to train and val order) is consistent.

    Args:
        path: base path (directory containing datasets).
        traces: list of traces to be used in training.
        mask: optional validity mask file to apply.
        pval: proportion of data to use for validation. Each `train` trace is
            split into two parts, with the first `1 - pval` being used for
            training, and the last `pval` being used for validation.
        ptrain: If specified, overrides `1 - pval` as the proportion used for
            training (e.g. to implement dataset size ablations).
        batch_size: train batch size. If multiple gpus are present (via
            `torch.cuda.device_count()`), the batch size is evenly divided
            between GPUs.
        val_samples: indices of (val) data to use as visualization samples. If
            `val_samples: int`, uses evenly spaced samples from the validation
            set (i.e. `linspace(0, len(val) - 1, val_samples)`).
        channels: channel specifications; see :py:mod:`deepradar.channels`.
        augmentations: data augmentation spec generators; see
            :py:mod:`deepradar.augmentations`.
        n_workers: number of workers to use; uses the number of cpus
            (i.e. `nproc`) by default (up to 32). Note that when
            `n_workers = 0`, the dataloader is run in the main thread to allow
            debuggers to work properly.
    """

    def __init__(
        self, path: str, traces: list[str] = [], mask: Optional[str] = None,
        pval: float = 0.2, ptrain: Optional[float] = None,
        batch_size: int = 64, val_samples: int | Iterable[int] = 16,
        channels: dict[str, dict] = {},
        augmentations: dict[str, dict] = {}, n_workers: Optional[int] = None
    ) -> None:
        super().__init__()
        self.base = path
        self.traces = sorted(traces)
        self.mask = mask
        self.pval = pval
        self.ptrain = (1 - pval if ptrain is None else ptrain)

        self._augmentations = {
            k: getattr(mod_augmentations, v["name"])(**v["args"])
            for k, v in augmentations.items()}
        self._paths = [os.path.join(self.base, t) for t in self.traces]
        self._channels = channels

        self.batch_size = batch_size // torch.cuda.device_count()

        if n_workers is None:
            self.nproc = min(32, multiprocessing.cpu_count())
        else:
            self.nproc = n_workers

        self._val_samples = val_samples

    def train_dataloader(self) -> DataLoader:
        """Get train dataloader (lightning API).

        Returns:
            Single training dataloader, with all traces batched and shuffled.
        """
        ds = RoverData(
            self._paths, channels=self._channels, mask=self.mask,
            augmentations=self._augmentations, bounds=(0.0, self.ptrain))
        return DataLoader(
            ds, batch_size=self.batch_size, shuffle=True, drop_last=True,
            num_workers=self.nproc, pin_memory=True)

    def val_dataloader(self) -> DataLoader:
        """Get val dataloader (lightning API).

        Returns:
            Single validation dataloader, with all traces concatenated.
        """
        ds = RoverData(
            self._paths, channels=self._channels, mask=self.mask,
            augmentations={}, bounds=(1.0 - self.pval, 1.0))
        return DataLoader(
            ds, batch_size=self.batch_size, shuffle=False, drop_last=True,
            num_workers=self.nproc, pin_memory=True)

    def eval_dataloader(self, path: str, batch_size: int = 16) -> DataLoader:
        """Create evaluation dataloader.

        Args:
            path: path to evaluation trace, relative to the base path.
            batch_size: batch size to use.

        Returns:
            Dataloader for this evaluation trace.
        """
        ds = RoverData(
            [os.path.join(self.base, path)],
            channels=self._channels, augmentations={})
        return DataLoader(
            ds, batch_size=batch_size, shuffle=False, drop_last=False,
            num_workers=self.nproc, pin_memory=True)

    @cached_property
    def val_samples(self) -> dict[str, Shaped[np.ndarray, "..."]]:
        """Get specific validation samples for validation visualizations.

        Returns:
            Data samples in the same batch format as normal dataloading, with
            the specified indices.
        """
        ds = RoverData(
            self._paths, channels=self._channels, augmentations={},
            bounds=(1.0 - self.pval, 1.0))
        if isinstance(self._val_samples, int):
            self._val_samples = np.linspace(
                0, len(ds) - 1, self._val_samples, dtype=np.uint32)

        samples = [ds[i] for i in self._val_samples]
        return {k: np.stack([s[k] for s in samples]) for k in samples[0]}
