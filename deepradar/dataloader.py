"""Lightning-based dataloader for a collection of Rover traces.

The primary API used should be the `RoverDataModule`, which can be fully
deserialized from a `**config` json-compatible spec. For example::

    dataset = RoverDataModule(**cfg, path="path/to/traces")

Key configuration options for `cfg`:

- **batch_size**: data batch size.
- **path**: all traces should be put in a directory `path`.
- **traces**: traces to use for training/validation.
- **pval**: the last `pval` (`0 < pval < 1`) of each trace is reserved for
  validation, with the first `1 - pval` being used for training.
- **val_samples**: fixed indices from the validation set to render
  visualizations for on each validation round.
- **transform**: transform spec. Keys correspond to sensors, while values are
  a list of transforms to apply (see :class:`TransformSpec`).
- **augment**: data augmentation parameter generators. Key correspond to
  possible augmentations; see :py:mod:`deepradar.transforms`.

Example configuration::

    {
        "batch_size": 64,
        "train": ["cic1", "cic2"],
        "pval": 0.2,
        "val_samples": [100, 200, 300, 400],
        "transform": {
            "lidar": [
                {"args": {}, "name": "Destagger"},
                {"args": {}, "name": "Map2D"}],
            "radar": [
                {"args": {}, "name": "AssertTx2"},
                {"args": {}, "name": "IIQQtoIQ"},
                {"args": {"pad": 0, "axes": [0, 1, 2, 3]}, "name": "FFTArray"},
                {"args": {}, "name": "ComplexPhase"}]
        },
        "augment": {
            "azimuth_flip": {"name": "Bernoulli", "args": {"p": 0.5}},
            "range_scale": {"name": "TruncatedLogNormal", "args": {
                "p": 0.5, "std": 0.2, "clip": 2.0}}
        }
    }
"""

import os, json
import multiprocessing
import numpy as np
from functools import partial, cached_property

from torch.utils.data import Dataset, DataLoader
import lightning as L

from beartype.typing import Union, TypedDict, Callable, Any, Iterable, Optional
from jaxtyping import Num

from . import transforms, augmentations


#: Any type which can be used as an index
Index = Union[np.integer, int]

#: Augmentation spec generator type
Augmentation = Callable[[], Any]


#: A Transform is a callable which takes the dataset path.
Transform = Callable[[str], transforms.BaseTransform]


class TransformSpec(TypedDict):
    """Transform specification."""

    name: str
    """Transform name.
    
    Should correspond to a `BaseTransform` in `transforms`.
    """

    args: dict[str, Any]
    """Parameters to pass to the specified `BaseTransform`."""


class RawChannel:
    """Generic sensor stream."""

    def __init__(self, path: str, channel: str) -> None:

        with open(os.path.join(path, 'meta.json')) as f:
            cfg = json.load(f)

        assert cfg[channel]['format'] == 'raw'
        self.dtype = np.dtype(cfg[channel]['type'])
        self.shape = cfg[channel]['shape']
        self.stride = np.prod(self.shape) * self.dtype.itemsize
        self.path = os.path.join(path, channel)

    def __getitem__(self, idx: Index) -> Num[np.ndarray, "..."]:
        with open(os.path.join(self.path), 'rb') as f:
            f.seek(self.stride * idx)
            raw = f.read(self.stride)
        return np.frombuffer(raw, dtype=self.dtype).reshape(self.shape)


class MetaChannel:
    """Sensor metadata dummy data."""

    def __init__(self) -> None:
        pass

    def __getitem__(self, idx: Index) -> Num[np.ndarray, "..."]:
        return np.array(())


class RoverTrace:
    """Single rover trace.
    
    Args:
        path: path to trace.
        indices: indices to use; generated by `roverp align`.
        transform: data transformations to perform.
        augment: data augmentation parameter generators.
        bounds: portion of the trace to use; if not specified, the entire trace
            is used. The bounds are specified as fractions of the dataset, e.g.
            `(0.0, 0.8)` for the first 80% of the trace and `(0.8, 1.0)` for
            the last 20%.
    """

    def __init__(
        self, path: str, indices: str = "_fusion/indices.npz",
        transform: dict[str, list[Transform]] = {},
        augment: dict[str, Augmentation] = {},
        bounds: Optional[tuple[float, float]] = None
    ) -> None:
        npz = np.load(os.path.join(path, indices))
        self.indices = npz["indices"]

        if bounds is not None:
            left = int(bounds[0] * self.indices.shape[0])
            right = int(bounds[1] * self.indices.shape[0])
            self.indices = self.indices[left:right]

        self.channel_names = {
            n: i for i, n in enumerate(npz["sensors"])}
        self.transform = {
            k: [tf(path) for tf in v] for k, v in transform.items()}
        self.augment = augment

        self.channels: dict[str, Union[RawChannel, MetaChannel]] = {
            "radar": RawChannel(os.path.join(path, "radar"), "iq"),
            "lidar": RawChannel(os.path.join(path, "_lidar"), "rng"),
            "meta": MetaChannel()}

    def __len__(self) -> int:
        return len(self.indices)

    def __getitem__(self, idx: Index) -> dict[str, Num[np.ndarray, "..."]]:
        aug = {k: v() for k, v in self.augment.items()}

        def apply_transform(k, data):
            for tf in self.transform.get(k, []):
                data = tf(data, aug=aug)
            return data

        def _get_index(idx, k):
            if k == "meta":
                return 0
            else:
                return self.indices[idx, self.channel_names[k]]

        return {
            k: apply_transform(k, v[_get_index(idx, k)])
            for k, v in self.channels.items()}


class RoverData(Dataset):
    """Collection of rover traces.
    
    Args:
        paths: list of dataset paths to include.
        kwargs: forwarded to each :class:`.RoverTrace`.
    """

    def __init__(self, paths: list[str], **kwargs) -> None:
        self.traces = [RoverTrace(p, **kwargs) for p in paths]
        self.indices = np.concatenate([
            [0], np.cumsum([len(t) for t in self.traces])]) - 1

    def __len__(self) -> int:
        return sum(len(t) for t in self.traces)

    def __getitem__(self, idx: Index) -> dict[str, Num[np.ndarray, "..."]]:
        if idx < 0 or idx > self.indices[-1]:
            raise ValueError("Index out of bounds.")
        else:
            ii = np.searchsorted(self.indices, idx) - 1
            return self.traces[ii][idx - self.indices[ii] - 1]


class RoverDataModule(L.LightningDataModule):
    """Rover dataloaders.
    
    Args:
        path: path to directory containing datasets
        traces: list of traces to be used in training
        pval: proportion of data to use for validation. Each `train` trace is
            split into two parts, with the first `1 - pval` being used for
            training, and the last `pval` being used for validation.
        transform: data transformations to perform.
        augment: data augmentation parameter generators.
        batch_size: train batch size.
        debug: whether to run in debug mode. When `debug=True`, use 
            `num_workers=0` (run dataloaders in main thread) to allow debuggers
            to work properly; otherwise, uses `num_workers=nproc`.
        val_samples: indices of (val) data to use as visualization samples.
    """

    def __init__(self,
        path: str, traces: list[str] = [], pval: float = 0.2,
        transform: dict[str, list[Union[Transform, TransformSpec]]] = {},
        augment: dict[str, Union[Augmentation, TransformSpec]] = {},
        batch_size: int = 64, val_samples: Iterable[int] = [0, 1, 2, 3],
        debug: bool = False
    ) -> None:
        super().__init__()
        self.base = path
        self.traces = traces
        self.pval = pval

        self._transform = {
            k: [self.as_transform(t) for t in v] for k, v in transform.items()}
        self._paths = [os.path.join(self.base, t) for t in self.traces]
        self._aug = {k: self.as_augmentation(v) for k, v in augment.items()}

        self.batch_size = batch_size
        self.nproc = 0 if debug else multiprocessing.cpu_count()
        self._val_samples = val_samples

    def train_dataloader(self) -> DataLoader:
        """Get train dataloader (lightning API)."""
        ds = RoverData(
            self._paths, transform=self._transform, augment=self._aug,
            bounds=(0.0, 1.0 - self.pval))
        return DataLoader(
            ds, batch_size=self.batch_size, shuffle=True, drop_last=True,
            num_workers=self.nproc)

    def val_dataloader(self) -> DataLoader:
        """Get val dataloader (lightning API)."""
        ds = RoverData(
            self._paths, transform=self._transform, augment={},
            bounds=(1.0 - self.pval, 1.0))
        return DataLoader(
            ds, batch_size=self.batch_size, shuffle=False, drop_last=True,
            num_workers=self.nproc)

    @cached_property
    def val_samples(self) -> dict[str, Num[np.ndarray, "..."]]:
        """Get specific validation samples for validation visualizations.

        Returns:
            Data samples in the same batch format as normal dataloading, with
            the specified indices.
        """
        ds = RoverData(
            self._paths, transform=self._transform, augment={},
            bounds=(1.0 - self.pval, 1.0))
        samples = [ds[i] for i in self._val_samples]
        return {k: np.stack([s[k] for s in samples]) for k in samples[0]}

    @staticmethod
    def as_transform(spec: Union[Transform, TransformSpec]) -> Transform:
        """Deserialize TransformSpec to transform if required."""
        if isinstance(spec, dict):
            return partial(getattr(transforms, spec["name"]), **spec["args"])
        else:
            return spec

    @staticmethod
    def as_augmentation(
        spec: Union[Augmentation, TransformSpec]
    ) -> Augmentation:
        """Deserialize TransformSpec to augmentation callable if required."""
        if isinstance(spec, dict):
            return getattr(augmentations, spec["name"])(**spec["args"])
        else:
            return spec
