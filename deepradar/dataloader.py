"""Lightning-based dataloader for a collection of Rover traces.

The primary API used should be the `RoverDataModule`, which can be fully
deserialized from a `**config` json-compatible spec. For example::

    dataset = RoverDataModule(**cfg, path="path/to/traces")

Key configuration options for `cfg`:

- **batch_size**: data batch size.
- **path**: all traces should be put in a directory `path`.
- **train, val**: traces to use in the train set, and val set.
- **transform**: transform spec. Keys correspond to sensors, while values are
  a list of transforms to apply (see :class:`TransformSpec`).
- **augment**: data augmentation parameter generators. Key correspond to
  possible augmentations; see :py:mod:`deepradar.transforms`.

Example configuration::

    {
        "batch_size": 64,
        "train": ["cic1", "cic2"],
        "val": ["cic4"],
        "transform": {
            "lidar": [
                {"args": {}, "name": "Destagger"},
                {"args": {}, "name": "Map2D"}],
            "radar": [
                {"args": {}, "name": "AssertTx2"},
                {"args": {}, "name": "IIQQtoIQ"},
                {"args": {"pad": 0, "axes": [0, 1, 2, 3]}, "name": "FFTArray"},
                {"args": {}, "name": "ComplexPhase"}]
        },
        "augment": {
            "azimuth_flip": {"name": "Bernoulli", "args": {"p": 0.5}},
            "range_scale": {"name": "TruncatedLogNormal", "args": {
                "p": 0.5, "std": 0.2, "clip": 2.0}}
        }
    }
"""

import os, json
import multiprocessing
import numpy as np
from functools import partial, cached_property

from torch.utils.data import Dataset, DataLoader
import lightning as L

from beartype.typing import Union, TypedDict, Callable, Any, Iterable
from jaxtyping import Num

from . import transforms, augmentations


#: Any type which can be used as an index
Index = Union[np.integer, int]

#: Augmentation spec generator type
Augmentation = Callable[[], Any]


#: A Transform is a callable which takes the dataset path.
Transform = Callable[[str], transforms.BaseTransform]


class TransformSpec(TypedDict):
    """Transform specification."""

    name: str
    """Transform name.
    
    Should correspond to a `BaseTransform` in `transforms`.
    """

    args: dict[str, Any]
    """Parameters to pass to the specified `BaseTransform`."""


class RawChannel:
    """Generic sensor stream."""

    def __init__(self, path: str, channel: str) -> None:

        with open(os.path.join(path, 'meta.json')) as f:
            cfg = json.load(f)

        assert cfg[channel]['format'] == 'raw'
        self.dtype = np.dtype(cfg[channel]['type'])
        self.shape = cfg[channel]['shape']
        self.stride = np.prod(self.shape) * self.dtype.itemsize
        self.path = os.path.join(path, channel)

    def __getitem__(self, idx: Index) -> Num[np.ndarray, "..."]:
        with open(os.path.join(self.path), 'rb') as f:
            f.seek(self.stride * idx)
            raw = f.read(self.stride)
        return np.frombuffer(raw, dtype=self.dtype).reshape(self.shape)


class RoverTrace:
    """Single rover trace.
    
    Args:
        path: path to trace.
        indices: indices to use; generated by `roverp align`.
        transform: data transformations to perform.
        augment: data augmentation parameter generators.
    """

    def __init__(
        self, path: str, indices: str = "_fusion/indices.npz",
        transform: dict[str, list[Transform]] = {},
        augment: dict[str, Augmentation] = {}
    ) -> None:
        npz = np.load(os.path.join(path, indices))
        self.indices = npz["indices"]
        self.channel_names = {
            n: i for i, n in enumerate(npz["sensors"])}
        self.transform = {
            k: [tf(path) for tf in v] for k, v in transform.items()}
        self.augment = augment

        self.channels = {
            "radar": RawChannel(os.path.join(path, "radar"), "iq"),
            "lidar": RawChannel(os.path.join(path, "_lidar"), "rng")}

    def __len__(self) -> int:
        return len(self.indices)

    def __getitem__(self, idx: Index) -> dict[str, Num[np.ndarray, "..."]]:
        aug = {k: v() for k, v in self.augment.items()}

        def apply_transform(k, data):
            for tf in self.transform.get(k, []):
                data = tf(data, aug=aug)
            return data

        return {
            k: apply_transform(k, v[self.indices[idx, self.channel_names[k]]])
            for k, v in self.channels.items()}


class RoverData(Dataset):
    """Collection of rover traces.
    
    Args:
        paths: list of dataset paths to include.
        indices: correspondence indices, as a subpath within each dataset.
        transform: transformations to apply to radar, lidar data.
        augment: data augmentation parameter generators.
    """

    def __init__(
        self, paths: list[str], indices: str = "_fusion/indices.npz",
        transform: dict[str, list[Transform]] = {},
        augment: dict[str, Augmentation] = {}
    ) -> None:
        self.traces = [
            RoverTrace(
                p, transform=transform, augment=augment, indices=indices
            ) for p in paths]
        self.indices = np.concatenate([
            [0], np.cumsum([len(t) for t in self.traces])]) - 1

    def __len__(self) -> int:
        return sum(len(t) for t in self.traces)

    def __getitem__(self, idx: Index) -> dict[str, Num[np.ndarray, "..."]]:
        if idx < 0 or idx > self.indices[-1]:
            raise ValueError("Index out of bounds.")
        else:
            ii = np.searchsorted(self.indices, idx) - 1
            return self.traces[ii][idx - self.indices[ii] - 1]


class RoverDataModule(L.LightningDataModule):
    """Rover dataloaders.
    
    Args:
        path: path to directory containing datasets
        train, val: list of traces to use as the train/val splits
        transform: data transformations to perform.
        augment: data augmentation parameter generators.
        batch_size: train batch size.
        debug: whether to run in debug mode. When `debug=True`, use 
            `num_workers=0` (run dataloaders in main thread) to allow debuggers
            to work properly; otherwise, uses `num_workers=nproc`.
        val_samples: indices of (val) data to use as visualization samples.
    """

    def __init__(self,
        path: str, train: list[str] = [], val: list[str] = [],
        transform: dict[str, list[Union[Transform, TransformSpec]]] = {},
        augment: dict[str, Union[Augmentation, TransformSpec]] = {},
        batch_size: int = 64, val_samples: Iterable[int] = [0, 1, 2, 3],
        debug: bool = False
    ) -> None:
        super().__init__()
        self.base = path
        self.train = train
        self.val = val
        self.transform = {
            k: [self.as_transform(t) for t in v] for k, v in transform.items()}
        self.augment = {
            k: self.as_augmentation(v) for k, v in augment.items()}

        self.batch_size = batch_size
        self.nproc = 0 if debug else multiprocessing.cpu_count()
        self._val_samples = val_samples

    @cached_property
    def val_samples(self) -> dict[str, Num[np.ndarray, "..."]]:
        """Get specific validation samples for validation visualizations.

        Returns:
            Data samples in the same batch format as normal dataloading, with
            the specified indices.
        """
        ds = RoverData(
            [os.path.join(self.base, t) for t in self.val],
            transform=self.transform, augment={})
        samples = [ds[i] for i in self._val_samples]
        return {k: np.stack([s[k] for s in samples]) for k in samples[0]}

    @staticmethod
    def as_transform(spec: Union[Transform, TransformSpec]) -> Transform:
        """Deserialize TransformSpec to transform if required."""
        if isinstance(spec, dict):
            return partial(getattr(transforms, spec["name"]), **spec["args"])
        else:
            return spec

    @staticmethod
    def as_augmentation(
        spec: Union[Augmentation, TransformSpec]
    ) -> Augmentation:
        """Deserialize TransformSpec to augmentation callable if required."""
        if isinstance(spec, dict):
            return getattr(augmentations, spec["name"])(**spec["args"])
        else:
            return spec

    def train_dataloader(self) -> DataLoader:
        """Get train dataloader (lightning API)."""
        ds = RoverData(
            [os.path.join(self.base, t) for t in self.train],
            transform=self.transform, augment=self.augment)
        return DataLoader(
            ds, batch_size=self.batch_size, shuffle=True, drop_last=True,
            num_workers=self.nproc)

    def val_dataloader(self) -> DataLoader:
        """Get val dataloader (lightning API)."""
        ds = RoverData(
            [os.path.join(self.base, t) for t in self.val],
            transform=self.transform, augment={})
        return DataLoader(
            ds, batch_size=self.batch_size, shuffle=False, drop_last=True,
            num_workers=self.nproc)
